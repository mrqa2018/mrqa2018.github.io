---
layout: main
title: Home
order: 1
---
**New**: we have released our list of [accepted papers](papers)!

Machine Reading for Question Answering (MRQA) has become an important testbed for 
evaluating how well computer systems understand human language,
as well as a crucial technology for industry applications such as search engines and dialog systems.
The research community has recently created a multitude of large-scale datasets 
over text sources such as 
Wikipedia ([WikiReading](http://www.aclweb.org/anthology/P16-1145), 
[SQuAD](https://aclweb.org/anthology/D16-1264),
[WikiHop](https://arxiv.org/pdf/1710.06481.pdf)), 
news and other articles ([CNN/Daily Mail](https://arxiv.org/pdf/1506.03340.pdf), 
[NewsQA](https://arxiv.org/pdf/1611.09830.pdf),
[RACE](http://aclweb.org/anthology/D17-1082)),
fictional stories ([MCTest](http://aclweb.org/anthology/D/D13/D13-1020.pdf), 
[CBT](https://arxiv.org/pdf/1511.02301.pdf),
[NarrativeQA](https://arxiv.org/pdf/1712.07040.pdf)), 
and general web sources ([MS MARCO](https://arxiv.org/pdf/1611.09268.pdf), 
[TriviaQA](http://www.aclweb.org/anthology/P17-1147), 
[SearchQA](https://arxiv.org/pdf/1704.05179.pdf)).
These new datasets have in turn inspired an even wider array of new question answering systems.

This workshop will gather researchers to address and discuss important research topics
surrounding MRQA, including:
- **Accuracy**: How can we make MRQA systems more accurate?
- **Interpretability**: How can systems provide rationales for their predictions?
- **Speed and Scalability**: How can systems scale to consider larger contexts, from long documents to the whole web?
- **Robustness**: How can systems generalize to other datasets and settings beyond the training distribution?
- **Dataset Creation**: What are effective methods for building new MRQA datasets?
- **Dataset Analysis**: What challenges do current MRQA datasets pose?
- **Error Analysis**: What types of questions or documents are particularly challenging for existing systems?

## Invited Speakers
Confirmed speakers:
- [Antoine Bordes](https://research.fb.com/people/bordes-antoine/), Facebook AI Research 
- [Jianfeng Gao](https://www.microsoft.com/en-us/research/people/jfgao/), Microsoft Research
- [Hannaneh Hajishirzi](http://ssli.ee.washington.edu/~hannaneh/), University of Washington
- [Sebastian Riedel](http://www.riedelcastro.org/), University College London
- [Richard Socher](https://www.socher.org/), Salesforce Research

## Important Dates
- ~~Deadline for submission: Monday, April 23, 2018~~  
- ~~Notification of acceptance: Tuesday, May 15, 2018~~  
- Deadline for camera-ready version: Monday, May 28, 2018  
- [Early registration deadline](https://acl2018.org/registration): June 4, 2018  
- Workshop Date: Thursday, July 19, 2018

All submission deadlines are 11:59 PM GMT -12 (anywhere in the world). 

## Organization
Steering Committee:
- [Antoine Bordes](https://research.fb.com/people/bordes-antoine/), Facebook AI Research
- [Percy Liang](https://cs.stanford.edu/~pliang/), Stanford University
- [Luke Zettlemoyer](https://www.cs.washington.edu/people/faculty/lsz), University of Washington

Organizing Committee:
- [Eunsol Choi](https://homes.cs.washington.edu/~eunsol/home.html), University of Washington
- [Minjoon Seo](https://seominjoon.github.io/), University of Washington
- [Danqi Chen](http://cs.stanford.edu/people/danqi/), Stanford University
- [Robin Jia](http://stanford.edu/~robinjia/), Stanford University 
- [Jonathan Berant](http://www.cs.tau.ac.il/~joberant/), Tel-Aviv University

## Sponsors
![Naver]({{ "/assets/images/naver-logo.png" | absolute_url }})

![Facebook]({{ "/assets/images/facebook-logo.png" | absolute_url }})


