---
title: "MRQA 2018"
bg: white
color: black

---
# Machine Reading for Question Answering (MRQA)
### Proposed Workshop for ACL/EMNLP/NAACL
### Date: TBD in 2018
### Contact: <mrqa2018@googlegroups.com>

Machine Reading for Question Answering (MRQA) has become an important testbed 
for evaluating the abilities of computer systems to understand human language,
and a crucial technology for many industrial applications such as search engines and dialog agents.
In this setting, a computer system is asked to answer questions about a natural language document.
Successful MRQA systems must understand syntactic attachments, coreference links, world knowledge, and other key phenomenon. 
Recognizing the potential of question answering as a comprehensive benchmark, 
the research community has recently witnessed a proliferation of new question answering datasets over 
text sources such as Wikipedia (WikiReading, SQuAD), news articles (CNN/Daily Mail, NewsQA), 
fictional stories (MCTest, CBT), and general web sources (MS MARCO, TriviaQA). 
These new datasets have in turn inspired an even wider array of new question answering systems.

Despite this rapid progress, 
there is yet much to understand about these datasets and systems. 
It is still difficult to agree on "good" and "bad" practices among various approaches, 
and current work on model development focuses primarily on improving the test accuracy of models trained on in-domain data, 
which may not measure other desirable system properties. 
Evaluating systems in this way obscures many other important desiderata, 
including model interpretability, robustness to distributional shift, and adequate modeling of various fine-grained phenomenon. 
Similarly, the diversity of existing datasets necessitates a more systematic evaluation of the datasets themselves.

Our goal with this workshop is to motivate researchers in the field to address and discuss 
the challenging questions surrounding the evaluation and understanding of MRQA systems and datasets. 
Currently, reading comprehension models, often including deep learning methods, 
are presented in distributed venues such as ICLR, ACL and NIPS/ICML, 
and a venue for discussion will benefit the community. 
Specifically, we seek submissions in the following areas:
- Interpretability: Can models provide a rationale for its predictions?
- Speed / Scalability: Can models scale to consider multiple, lengthy documents or sets of answers?
- Robustness: How can we test how easily the model can generalize to other datasets/settings?
- Analysis / evaluation of QA datasets: Can we quantify the challenges posed by each datasets?
- Analysis of model predictions: Are particular types of questions challenging for existing systems?
- New QA datasets / tasks
- New QA models

